%\def\CTeXPreproc{Created by ctex v0.2.12, don't edit!}\documentclass[12pt,draftcls,onecolumn]{IEEEtran}
%\documentclass[12pt,draftcls,twocolumn]{IEEEtran}
%\documentclass[journal]{IEEEtran}
\documentclass[12pt,onecolumn,journal]{IEEEtran}
\usepackage{setspace}
\singlespacing
\usepackage{amssymb,amsmath,amsthm}
\usepackage{enumerate}
\usepackage{color}
%\usepackage[mathscr]{eucal}
%\usepackage{amssymb}
%\usepackage{amsmath}
%\usepackage{subfigure}
%\usepackage[dvips]{graphicx}
%\usepackage{epsfig}
%\usepackage{graphicx}
\usepackage{cases}
\usepackage{amsfonts}
\usepackage{graphicx}
%\usepackage{amssymb}
\usepackage{booktabs}

\usepackage{array}

\graphicspath{{./}{Fig/}}

\newcommand{\BOX}{\hfill $\Box$}
\newcommand{\NAB}{\hfill $\nabla \nabla \nabla$}
\newcommand{\den}{\stackrel{\Delta}{=}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\hs}{\hspace}
\newcommand{\vs}{\vspace}
\newcommand{\lt}{\left}
\newcommand{\rt}{\right}
\newcommand{\bib}{\bibitem}
\newcommand{\ds}{\displaystyle}
\newcommand{\fc}{\frac}
\newcommand{\ol}{\overline}
\newcommand{\da}{\Delta}
\newcommand{\nm}{\nonumber}
\def\is{i \in \mathcal{S}}
\def\s{\sum_{i=1}^mh_i}
\def\j{\sum_{j=1}^mh_j}
\def\S{\sum_{i=1}^mh_i^+}
\def\ss{\sum_{i=1}^m\sum_{j=1}^mh_ih_j}
\def\ssss{\sum_{i=1}^m\sum_{j=1}^m\sum_{p=1}^m\sum_{q=1}^mh_ih_jh_ph_q}
\def\sf{\sum_{i=1}^m\sum_{j=1}^m\sum_{l=1}^m\sum_{p=1}^m\sum_{q=1}^mh_ih_jh_l^+h_ph_q}
\def\str{\sum_{i=1}^m\sum_{j=1}^m\sum_{l=1}^mh_ih_jh_l^+}
\def\st{\sum_{i=1}^m\sum_{l=1}^mh_i^2h_l^+}
\def\sij{\sum_{i=1}^{m-1}\sum_{j=i+1}^m\sum_{l=1}^mh_ih_jh_l^+}
%\makeatletter
%\def\EquationsBySection{\def\theequation{\thesection.\arabic{equation}}%
% \@addtoreset{equation}{section}}
\newtheorem{definition}{Definition}%[section]
\newtheorem{algorithm}{Algorithm}%[section]
\newtheorem{lemma}{Lemma}%[section]
\newtheorem{corollary}{Corollary}%[section]
\newtheorem{proposition}{Proposition}%[section]
\newtheorem{theorem}{Theorem}%[section]
\newtheorem{remark}{Remark}%[section]
\newtheorem{example}{Example}%[section]
\newtheorem{assumption}{Assumption}%[section]
\newtheorem{notation}{Notation}%[section]
\newcommand{\RNum}[1]{\expandafter{\romannumeral #1\relax}}
%\makeatother \EquationsBySection
\begin{document}
\title{\LARGE The Homework of Chapter 1}
\author{ }
\maketitle
\vspace{7cm}
\begin{center}%
{
\Large
\begin{tabular}{p{14em}l@{\extracolsep{0.5em}}l}
\qquad Name: Zean Bao\\
\qquad Student Number:3120200881\\
\end{tabular}
}
\end{center}
\newpage
\textbf{Pro. 1} (a) A gambler has in his pocket a fair coin and a two-headed coin. He selects one of the coins at random, when he flips it, it shows heads. What is the probability that it is the fair coin? (b) Suppose that he flips the same coin a second time and again it shows heads. Now what is the probability that it is the fair coin? (c) Suppose that he flips the same coin a third time and it shows tails. Now what is the probability that it is the fair coin?
\\
\textbf{Solution:} \\
\textcolor[rgb]{1,0,0}
{
(a):\\
from Bayes' fromula, we have
\[\begin{aligned}
P(fair|heads)&=\frac{P(heads|fair)P(fair)}{P(heads|fair)P(fair)+P(heads|two-headed)P(two-headed)}& \\
&=\frac{1/2.1/2}{1/2.1/2+1.1/2}=\frac{1}{3}&
\end{aligned}
\]
(b):\\
from Bayes' formula, we have
\[\begin{aligned}
P(fair|head\&head)&=\frac{P(head\&head|fair)P(fair)}{P(head\&head|fair)P(fair)+P(head\&head|two-headed)P(two-headed)}& \\
&=\frac{1/4.1/2}{1/4.1/2+1.1/2}=\frac{1}{5}&
\end{aligned}
\]
(c):\\
since it is impossible to flip the two-headed coin adn get tails, the current probability that it is fair coin is 1.
}
\\
\\
\textbf{Pro. 2} Three prisoners are informed by their jailer that one of them has been chosen at random to be executed, and the other two are to be freed. Prisoner $A$ asks the jailer to tell him privately which of his fellow prisoners will be set free, claiming that there would be no harm in divulging this information, since he already knows that at least one will go free. The jailer refuses to answer this question, pointing out that if $A$ knew which of his fellows were to be set free, then his own probability of being executed would rise from $\frac{1}{3}$ to $\frac{1}{2}$, since he would then be one of two prisoners. What do you think of the jailer's reasoning?\\
\textbf{Solution:}
\\
\textcolor[rgb]{1,0,0}
{
without loss of generality,Suppose that the jailer told Prisoner A that Priosner B would be set free.
Then by Baye's formula,we have:
\[
\begin{aligned}
P(A dies\mid told B)
&=\frac{P(told B|A dies)P(A dies)}{P(told B|A dies)P(A dies)+P(told B|B dies)P(B dies)+P(told B|C dies)P(C dies)}&
\end{aligned}
\]
if A is to die,he could be told either B is to be freed or C is to be freed,each with probability 1/2.
Thus $P(told B|A dies)=1/2$.if B is to die,A would not be told B is to be freed,Thus $P(told B|B dies)=0$.if C is to die,A must be told B is to be freed.
Thus $P(told B|C dies)=1$.so we have
\[
\begin{aligned}
P(A dies\mid told B)
&=\frac{P(told B|A dies)P(A dies)}{P(told B|A dies)P(A dies)+P(told B|B dies)P(B dies)+P(told B|C dies)P(C dies)}& \\
&=\frac{1/2.1/3}{1/2.1/3+0+1.1/3}& \\
&=\frac{1}{3}&\\
\end{aligned}
\]
So the jailer is wrong.Divulging this information will not change the probability that A will be executed.
}
\\
\\
\textbf{Pro. 3} Suppose we know that the number of items produced in a factory during a week is a random variable with mean $500$. What can be said about the probability that this week's production will be least 1000?
\\
\textbf{Solution}
\textcolor[rgb]{1,0,0}
{
From the Markov inequality
\[P\{X \geq a\} \leq \frac{E[x]}{a} \]
thus to say:
\[P\{X \geq 1000\} \leq \frac{500}{1000}=0.5\]
}
\\
\textbf{Pro. 4} There are $n$ types of coupons. Each newly obtained coupon is, independently, type $i$ with probability $p_i, i=1,\ldots,n$. Find the expected number and the variance of the number of distinct types obtained in a collection of $k$ coupons.
\\
\textbf{Solution}
\\
\textcolor[rgb]{1,0,0}
{
Let $X_i$ be a random Variable which is 1 if the type $i$ in the collection of k coupons and be 0 if it is not in.\\
The random variable $N=\sum_{i=1}^{n}X_i$\\
which can be described as:\\
\[
X_i=\left\{
\begin{aligned}
&1, {\rm type}\,i\,{\rm is\,in\,the\,collection}&\\
&0, {\rm otherwise}&\\
\end{aligned}
\right.    
\]
Now 
\[P(x_i=0)=P({\rm type}\,i\,{\rm is\,not\,in\,the\,collection})=(1-p_i)^k\]
\[E(N)=\sum_{i=1}^{n}E{X_i}=\sum_{i=1}^{n}(1-(1-p_i)^k)\]
for $i \neq j$:\\
\[
\begin{aligned}
1-P(X_i=1,X_j=1)&=P({X_i=0}\cup{X_j=0})&\\
&=P(X_i=0)+P(X_j=0)-P(X_i,X_j=0)&\\
&=(1-p_i)^k+(1-p_j)^k-(1-p_i-p_j)^k&\\
\end{aligned}
\]
so the variance of $N$ is:
\[Var(N)=Var(\sum_{i=1}^{n}X_i)=\sum_{i=1}^{n}Var(X_i)+2\sum_{i=1}^{n}\sum_{j<i}Cov(X_i,X_j)\]
hence:$E(X_i)=E(X^2_i)=1-(1-p_i)^k,E(X_iX_j)=(1-p_i)^k+(1-p_j)^k-(1-p_i-p_j)^k$\\
we already know:$Var(X_i)=E(X^{2}_i)-E^{2}(X_i)=(1-p_i)^k(1-(1-p_i)^k),Cov(X_i,X_j)=E(X_iX_j)-E(X_i)E(X_j)$
so,we have:
\[
\operatorname{Var}(N)=
\sum_{i=1}^{n}\left(1-p_{i}\right)^{k}\left(1-\left(1-p_{i}\right)^{k}\right)+
2 \sum_{1 \leq i, j \leq n}\left[\left(1-p_{i}-p_{j}\right)^{k}-\left(1-p_{i}\right)^{k}-\left(1-p_{j}\right)^{k}\right]    
\]
}
\\

\textbf{Pro. 5} Let $X_1, X_2, \ldots$ be a sequence of independent identically distributed continuous random variables. We say that a record accurs at time $n$ if $X_n>max(X_1,\ldots,X_{n-1})$. That is, $X_n$ is a record if it is larger than each of $X_1,\ldots,X_{n-1}$. Show\\
(i) $P$\{a record occurs at time n\}$=\frac{1}{n}$;\\
(ii) $E$[number of records by time n]$=\sum^n_{i=1}{\frac{1}{i}}$;\\
(iii) $Var$(number of records by time n)$=\sum^n_{i=1}{(i-1)/i^2}$;\\
(iv) Let $N$=min\{$n:n>1$ and a record occurs at time $n$\}. Show $E[N]=\infty$.
\\
\textbf{Solution}
\\
\textcolor[rgb]{1,0,0}
{
Let $R(n)$ be the event that $X_n$is a record.That is the the event for$X_n=max\{X_1,X_2,\cdots,X_n\}$\\
\[
R_i=\left\{
\begin{aligned}
&1, {\rm if}\,X_i=max\{X_1,X_2,\cdots,X_n\}&\\
&0, {\rm otherwise}&\\
\end{aligned}
\right.    
\]
(i): By symmetry each sample thereby has equal probability for being the maxiumum among the first two,
so $P(R_2)=P(X_2=max(X_1,X_2))=\frac{1}{2}$,\\
in the same $P(R_3)=P(X_3=max(X_1,X_2,X_3))=\frac{1}{3}$,\\
$\cdots$,\\
$P(R_n)=P({\rm a \, record \, occurs \, at \, time n})=P(max(X_1,X_2,\cdots,X_n))=\frac{1}{n}$\\
(ii):
Let $Q$ be the event that the number of records by time n,thus:
\[Q=\sum_{i=1}^{n}R_i\]
\[E(Q)=E(\sum_{i=1}^{n}R_i)=\sum_{i=1}^{n}E(1.R_i)=\sum_{i=1}^{n}\frac{1}{i}\]
(iii):
from (ii), we have:
\[Var(Q)=E(Q^2)-E^2(Q)\]
we already know $E(Q^2)=E(Q)$,
\[Var(Q)=\sum_{i=1}^{n}\frac{1}{i}-\sum_{i=1}^{n}\frac{1}{i^2}=\sum_{i=1}^{n}\frac{i-1}{i^2}\]
\RNum{4}:
The event that a record occurs at time $n$ but not before at times $2, \ldots, n-1$ is the event
$$
A_{n}=\left\{\max _{2 \leq k<n} X_{k}<X_{1}<X_{n}\right\}
$$
Its probability is $P\left(A_{n}\right)=\frac{1}{n(n-1)}$ by similar reasonings as above, so
$$
E N=\sum_{n=2}^{\infty} n P\left(A_{n}\right)=\sum_{n=2}^{\infty} \frac{1}{n-1}=\infty
$$
}


\textbf{Pro. 6} Consider a experiment which results in one of three possible outcome $i$ occurring with probability $p_i,i=1,2,3,\sum^3_{i=1}{p_i}=1$. Suppose that $n$ independent replications of this experiment are performed and let $X_i,i=1,2,3$, denote the number of times outcome $i$ appears. Determine the conditional expectation of $X_1$ given that $X_2=m$.\\
\\
\textbf{Solution}
\textecolor[rgb]{1,0,0}
{
For $k \leq n-m$:
\[P\{X_1=k|X_2=m\}=\]
}
\textbf{Pro. 7} The joint density of $X$ and $Y$ is given by
\begin{eqnarray*}%
f(x,y)=
               \begin{cases}
                \frac{1}{2}ye^{-xy},    & 0<x<\infty,0<y<2 \\
                 0,  & otherwise\\
               \end{cases}
\end{eqnarray*}
What is $E[e^{X/2}|Y=1]$?
\\
\textbf{Solution}
\textcolor[rgb]{1,0,0}
{
The conditional density of $X$,when $Y=1$ is:
\[f_{(X|Y)}(x|1)=\frac{f(x,1)}{f_Y(1)}=\frac{\frac{1}{2}e^{-x}}{\int_{0}^{\infty}\frac{1}{2}e^{-x}dx}=e^{-x}\]
hence:
\[E[e^{X/2}|Y=1]=\int_{0}^{\infty}f_{X|Y}(x|1)dx=\int_{0}^{\infty}e^{-x/2}e^x=2\]
}
\\
\textbf{Pro. 8} A miner is trapped in a mine containing three doors. The first door leads to a tunnel that takes him to safety after two hours of travel. The second door leads to a tunnel that returns him to the mine after three hours of travel. The third door leads to a tunnel that returns him to his mine after five hours. Assuming that the miner is at all times equally likely to choose any one of the doors, what is the expected length time until the miner reaches safty?
\\
\textbf{Solution};
\\
\textcolor[rgb]{1,0,0}
{
Let $X$ be the time that the miner needs to get to the safety,$Y$ be the door which he select.
\[
\begin{aligned}
E[X]=&E[X|Y=1]P\{Y=1\}+E[X|Y=2]P\{Y=2\}+E[X|Y=3]P\{Y=3\}&\\
=&\frac{1}{3}(E(X|Y=1)+E(X|Y=2)+E(X|Y=3))&\\
\end{aligned}
\]
In the last two cases, if he goes back to the original state, he is back to the original state,so,we have:
\[E[X|Y=1]=2,E[X|Y=2]=3+E[X],E[X|Y=3]=5+E[X]\]
so,we can calculate that:$E[X]=10$
}
\\
\textbf{Pro. 9} An automobile insurance company classifies each of its policyholder as being of the types $i=1,\ldots,k$. It supposes that the numbers of accidents that a type $i$ policyholder has in successive years are independent Poisson random variables with mean $\lambda_i,i=1,\ldots,k$. The probability that a newly insured policyholder is type $i$ is $p_i,\sum^k_{i=1}{p_i}=1$. Given that a policyholder had $n$ accidents in her first year, what is the expected number that she has in her second year? What is the conditional probability that she has $m$ accidents in her second year?\\
\textbf{Solution}
\\
\textbf{Pro. 10} Show that\\
\quad\quad(a)$E[XY|Y=y]=yE[X|Y=y]$\\
\quad\quad(b)$ E[g(X,Y)|Y=y]=E[g(X,y)|Y=y]$\\
\quad\quad(c)$ E[XY]=E[YE[X|Y]]$\\
\textbf{Proof}
\end{document}